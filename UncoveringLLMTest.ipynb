{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d92dfc8",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cac1c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIG_rewrites_path = \"./Methods/UncoveringLLM/rewrites/AIGCodeSet\"\n",
    "CodeMirage_rewrites_path = \"./Methods/UncoveringLLM/rewrites/CodeMirage\"\n",
    "SunEtAl_rewrites_path = \"./Methods/UncoveringLLM/rewrites/SunEtAl\"\n",
    "AIG_Dataset_path = \"./Dataset/AIGCodeSet/\"\n",
    "CodeMirage_path = \"./Dataset/CodeMirage/\"\n",
    "SunEtAl_path = \"./Dataset/SunEtAl/SunEtAlNew\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b0d9ae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "AIG = load_from_disk(AIG_Dataset_path)\n",
    "CodeMirage = load_from_disk(CodeMirage_path)\n",
    "SunEtAl = load_from_disk(SunEtAl_path)\\\n",
    "\n",
    "AIG_R= AIG[\"rightcode\"]\n",
    "AIG_W= AIG[\"wrongcode\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0707c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "\n",
    "def extract_assistant_content(example):\n",
    "    for m in example[\"messages\"]:\n",
    "        if m.get(\"role\") == \"assistant\":\n",
    "            return {\"assistant_content\": m.get(\"content\")}\n",
    "    return {\"assistant_content\": None}\n",
    "\n",
    "\n",
    "\n",
    "def adding_rewriting(ds, rewrites_path, idx=\"idx\", i=0):\n",
    "    \"\"\"\n",
    "    Returns: {datasets.Dataset}\n",
    "        The original dataset with additional columns `rewritedCode_0`, `rewritedCode_1`, etc.,\n",
    "        one for each rewrite file provided.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Sanity checks for required columns\n",
    "    if idx not in ds.column_names:\n",
    "        print(f\"{idx} is not inside ds\")\n",
    "\n",
    "\n",
    "    # Mapping function to add a single rewrite column to each row\n",
    "    def add_column(row, ds_map, i, idx):\n",
    "        rewrited = ds_map.get(row[idx])  # use .get() to avoid KeyError\n",
    "        return {f\"rewritedCode_{i}\": rewrited}\n",
    "\n",
    "    # Process each rewrite file\n",
    "    # Load rewrite JSONL as Dataset\n",
    "    ds_jsonl = load_dataset(\"json\", data_files=rewrites_path)[\"train\"]\n",
    "\n",
    "    # Flatten nested structures (e.g., metadata.index → top-level)\n",
    "    ds_jsonl = ds_jsonl.flatten()\n",
    "\n",
    "    ds_jsonl = ds_jsonl.map(extract_assistant_content)\n",
    "\n",
    "\n",
    "\n",
    "     # Select only the needed columns\n",
    "    ds_jsonl = ds_jsonl.select_columns([f\"metadata.{idx}\", \"assistant_content\"])\n",
    "\n",
    "    # Rename columns to match our convention\n",
    "    ds_jsonl = ds_jsonl.rename_column(f\"metadata.{idx}\", idx)\n",
    "    ds_jsonl = ds_jsonl.rename_column(\"assistant_content\", \"content\")\n",
    "\n",
    "    # Convert rewrite dataset to dict: index → content\n",
    "    ds_map = {row[idx]: row[\"content\"] for row in ds_jsonl}\n",
    "\n",
    "    # Add the rewrite column to the main dataset\n",
    "    ds = ds.map(add_column, fn_kwargs={\"ds_map\": ds_map, \"i\": i, \"idx\": idx})\n",
    "\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c6390b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['problem_id', 'submission_id', 'status_in_folder', 'LLM', 'code', 'ada_embedding', 'label', 'lines', 'code_lines', 'comments', 'functions', 'blank_lines', 'cleared_code', 'idx', 'rewritedCode_0', 'rewritedCode_1', 'rewritedCode_2', 'rewritedCode_3'],\n",
      "    num_rows: 249\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    AIG_R=adding_rewriting(AIG_R, \"./Methods/UncoveringLLM/rewrites/\" + str(i) + \"AIGCodeSet\" + \".jsonl\", i=i)\n",
    "print(AIG_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a931ac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    AIG_W=adding_rewriting(AIG_W, \"./Methods/UncoveringLLM/rewrites/\" + str(i) + \"AIGCodeSet\" + \".jsonl\", i=i)\n",
    "print(AIG_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80c9395",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@TODO\n",
    "for i in range(4):\n",
    "    CodeMirage=adding_rewriting(CodeMirage, \"./Methods/UncoveringLLM/rewrites/\" + str(i) + \"CodeMirage\" + \".jsonl\", i=i)\n",
    "print(CodeMirage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e1abdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@TODO\n",
    "for i in range(4):\n",
    "    SunEtAl=adding_rewriting(CodeMirage, CodeMirage_path + i + \".jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd57ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@TODO\n",
    "for i in range(4):\n",
    "    SunEtAl=adding_rewriting(SunEtAl, SunEtAl_path + i + \".jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2688d25",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4119f1",
   "metadata": {},
   "source": [
    "Model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9408c296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at Methods\\UncoveringLLM\\model and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch, torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "GC_Tokenizer = \"microsoft/graphcodebert-base\"  # same training backbone (GraphCodeBert) tokenizer\n",
    "GC_FineTuned =  Path(\"./Methods/UncoveringLLM/model/\") # trained model\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(GC_Tokenizer)\n",
    "encoder   = AutoModel.from_pretrained(GC_FineTuned).to(DEVICE).eval() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8cdfb171",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TODO: ultimi blocchi semantici per rientrare in lunghezza \n",
    "\n",
    "# una volta, dopo aver creato il tokenizer:\n",
    "tokenizer.truncation_side = \"left\"   # così conserva gli ULTIMI token\n",
    "\n",
    "def encode_codes(original_code: str, rewrites: list[str], max_len: int = 512):\n",
    "    # sanifica input (evita None → errore del tokenizer)\n",
    "    texts = [(original_code if isinstance(original_code, str) else \"\")]\n",
    "    texts += [(r if isinstance(r, str) else \"\") for r in list(rewrites)]\n",
    "\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # ❌ RIMUOVI QUESTA PARTE (enc è un dict, non si può tagliare)\n",
    "    # if len(enc) > max_len:\n",
    "    #     enc = enc[-max_len:]\n",
    "    #     print(\"!! We use only the last tokens !!\")\n",
    "\n",
    "    enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = encoder(**enc).last_hidden_state   # [B, L, H]\n",
    "        cls = out[:, 0, :]                       # [B, H]\n",
    "        cls = F.normalize(cls, p=2, dim=-1)\n",
    "\n",
    "    orig_emb = cls[0]         # [H]\n",
    "    rewrite_embs = cls[1:]    # [R, H]\n",
    "    return orig_emb, rewrite_embs\n",
    "\n",
    "\n",
    "\n",
    "# best number of rewrites is 4\n",
    "def detect(original_code: str, rewrites: list[str]):\n",
    "    if len(rewrites) != 4:\n",
    "        print(f\"Number of rewrites is not 4 but: {len(rewrites)}\")\n",
    "\n",
    "    e0, eR = encode_codes(original_code, rewrites)  # e0: [H], eR: [R, H]\n",
    "    sims = F.cosine_similarity(\n",
    "        eR, e0.unsqueeze(0).expand_as(eR), dim=-1\n",
    "    ).cpu().numpy()  # <-- fix qui\n",
    "    score = float(sims.mean()) if len(sims) else float(\"nan\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faf9819",
   "metadata": {},
   "source": [
    "# Start testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a7c17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "ds2 = load_from_disk(\"./Dataset/AIGCodeSet\")\n",
    "\n",
    "\n",
    "def evaluation(row):\n",
    "    score = detect(\n",
    "        row.get(\"code\") or \"\",\n",
    "        [\n",
    "            row.get(\"rewritedCode_0\") or \"\",\n",
    "            row.get(\"rewritedCode_1\") or \"\",\n",
    "            row.get(\"rewritedCode_2\") or \"\",   # <-- prima ripetevi _0, qui serve _2\n",
    "            row.get(\"rewritedCode_3\") or \"\",\n",
    "        ],\n",
    "    )\n",
    "    return {\"score\": score}\n",
    "\n",
    "AIG_RR = AIG_R.map(evaluation)\n",
    "AIG_WW = AIG_W.map(evaluation)\n",
    "\n",
    "print(AIG_RR)\n",
    "print(AIG_WW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "89194c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media score (label=1): 0.19803214692566767\n",
      "Media score (label=0): nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanat\\AppData\\Local\\Temp\\ipykernel_45884\\939602039.py:8: RuntimeWarning: Mean of empty slice.\n",
      "  media_label_0 = scores[labels == 0].mean()\n",
      "c:\\Programmazione\\python\\Lib\\site-packages\\numpy\\_core\\_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "scores = np.array(AIG_RR[\"score\"])\n",
    "labels = np.array(AIG_RR[\"label\"])\n",
    "\n",
    "media_label_1 = scores[labels == 1].mean()\n",
    "media_label_0 = scores[labels == 0].mean()\n",
    "\n",
    "print(\"Media score (label=1):\", media_label_1)\n",
    "print(\"Media score (label=0):\", media_label_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6985daa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media score (label=1): 0.2175175985284\n",
      "Media score (label=0): 0.22179606354918824\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mSi è verificato un arresto anomalo del Kernel durante l'esecuzione del codice nella cella attiva o in una cella precedente. \n",
      "\u001b[1;31mEsaminare il codice nelle celle per identificare una possibile causa dell'errore. \n",
      "\u001b[1;31mPer altre informazioni, fare clic<a href='https://aka.ms/vscodeJupyterKernelCrash'>qui</a>. \n",
      "\u001b[1;31mPer ulteriori dettagli, visualizzare Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "scores = np.array(AIG_WW[\"score\"])\n",
    "labels = np.array(AIG_WW[\"label\"])\n",
    "\n",
    "media_label_1 = scores[labels == 1].mean()\n",
    "media_label_0 = scores[labels == 0].mean()\n",
    "\n",
    "print(\"Media score (label=1):\", media_label_1)\n",
    "print(\"Media score (label=0):\", media_label_0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
