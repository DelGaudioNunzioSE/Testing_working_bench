{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35fee499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import datasets\n",
    "import argparse\n",
    "from typing import Tuple\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn import metrics\n",
    "import matplotlib as plt\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from torch.optim import lr_scheduler\n",
    "from typing import Callable, Dict, List, Tuple, Union\n",
    "import csv\n",
    "from timeit import default_timer as timer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#first data exploration script for datamining phase \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class stylometer_classifier(torch.nn.Module):\n",
    "    def __init__(self,pretrained_encoder,dimensionality):\n",
    "        super(stylometer_classifier, self).__init__()\n",
    "        self.modelBase = pretrained_encoder\n",
    "        self.pre_classifier = torch.nn.Linear(dimensionality, 768, dtype=torch.bfloat16)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.classifier = torch.nn.Linear(768, 1, dtype=torch.bfloat16)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, padding_mask):\n",
    "        output_1 = self.modelBase(input_ids=input_ids, attention_mask=padding_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        #Here i take only the cls token representation for further classification\n",
    "        cls_output = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(cls_output)\n",
    "        afterActivation = self.activation(pooler)\n",
    "        pooler_after_act = self.dropout(afterActivation)\n",
    "        output = self.classifier(pooler_after_act)\n",
    "\n",
    "        if output>=0.07:\n",
    "            return {\"my_class\":\"It's a Human!\",\n",
    "                   \"prob\":output}\n",
    "        else:\n",
    "            return {\"my_class\":\"It's an LLM!\",\n",
    "                   \"prob\":output}\n",
    "\n",
    "\n",
    "def adapt_model(model:object, dim:int=1024) -> object:\n",
    "    \"\"\"\n",
    "    This function returns the model with a classification head\n",
    "    \"\"\"\n",
    "    newModel = stylometer_classifier(model,dimensionality=dim)\n",
    "\n",
    "    return newModel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def inizialize(cache_dir = \"./Methods/IsThisYou/cache\",\n",
    "                model_name = \"Salesforce/codet5p-770m\",\n",
    "                path_checkpoint = \"./Methods/IsThisYou/checkpoint.bin\"):\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    #load tokenizer\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, token=None)\n",
    "\n",
    "    #loading model and tokenizer for functional translation\n",
    "    model = transformers.T5EncoderModel.from_pretrained(model_name).to(DEVICE)\n",
    "\n",
    "    #adding classification head to the model\n",
    "    model = adapt_model(model, dim=model.shared.embedding_dim).to(DEVICE)\n",
    "    model.load_state_dict(torch.load(path_checkpoint,map_location=DEVICE))\n",
    "    model = model.eval()\n",
    "\n",
    "    print(\"Welcome to the Human-AI stylomety tool, insert the code you want to inspect here, \\n you can end input with Ctrl+D (linux or mac) or Ctrl+Z and enter for windows, to exit the tool enter Ctl+C:  \\n\")\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def run(code, model, tokenizer):\n",
    "    tokenized_input = tokenizer(code)\n",
    "    out = model(torch.tensor(tokenized_input.input_ids),torch.tensor(tokenized_input.attention_mask))\n",
    "    print(\"\\n\",out[\"my_class\"],\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64e5e9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mSi Ã¨ verificato un arresto anomalo del Kernel durante l'esecuzione del codice nella cella attiva o in una cella precedente. \n",
      "\u001b[1;31mEsaminare il codice nelle celle per identificare una possibile causa dell'errore. \n",
      "\u001b[1;31mPer altre informazioni, fare clic<a href='https://aka.ms/vscodeJupyterKernelCrash'>qui</a>. \n",
      "\u001b[1;31mPer ulteriori dettagli, visualizzare Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5p-770m\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Salesforce/codet5p-770m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c29965",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = inizialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d93eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(code = \"def somma(a, b): return a + b\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d887b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_snippet = \"def somma(a, b): return a + b\"\n",
    "inputs = tokenizer([code_snippet], return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    out = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "print(out)  # {'label': 'Human'/'LLM', 'prob': ...}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
